{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_11a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We will use the IMDB dataset that consists of 50,000 labeled reviews of movies (positive or negative) and 50,000 unlabelled ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('/home/kienmn/.fastai/data/imdb/tmp_clas'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/ll_clas.pkl'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/wt103_tiny.tgz'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/vocab_lm.pkl'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/ll_lm.pkl'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/ll_class.pkl'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/ld.pkl'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/train'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/test'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/tmp_lm'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/imdb.vocab'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/pretrained'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/unsup'),\n",
       " Path('/home/kienmn/.fastai/data/imdb/README')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a subclass of `ItemList` that will read the texts in the corresponding filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_file(fn):\n",
    "  with open(fn, 'r', encoding='utf8') as f:\n",
    "    return f.read()\n",
    "  \n",
    "class TextList(ItemList):\n",
    "  @classmethod\n",
    "  def from_files(cls, path, extensions='.txt', recurse=True, include=None, **kwargs):\n",
    "    return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n",
    "  \n",
    "  def get(self, i):\n",
    "    if isinstance(i, Path):\n",
    "      return read_file(i)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mItemList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mItemList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mListContainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34mf'{super().__repr__()}\\nPath: {self.path}'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m      \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mcompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Workspace/JupyterWorkspace/fastai_part2/exp/nb_08.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     ImageList, TextList\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ItemList??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(il.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first one as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'ve gotta say, I usually like horror movies that i\\'ve never seen... however, this one was just to pathetic for my gory taste. I\\'m used to the gory, gut wrenching types... but this particular movie was lame. The acting was horrible (yet the corny (no pun intended) one-liners were cute). And the sequel to it, Scarecrow Slayer was even worse! Yes, probably, when it first came out, there was a huge rave about it and people liked it. But when movies like The Ring and The Exorcist of Emily Rose come out, movies like these make movies like Scarecrow seem childish. If you want a movie to just pass the time, pick this one! The special effects are cheesy as heck. But seeing that it was a low budget movie, I can kind of see where that would come in. This will kind of remind you of the movie \"Children Of The Corn.\" Independent movies rock.... most of the time. So if you want to see a scarecrow killing people with corncobs, or in the sequel, 2 scarecrows going at it, then these movies would be for you.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = il[0]\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text classification, we will split by the grand parent folder as before, but for language modeling, we take all the texts and just put 10% aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitData\n",
       "Train: TextList (90000 items)\n",
       "[Path('/home/kienmn/.fastai/data/imdb/train/neg/3072_4.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/10395_2.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/9280_1.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/2771_1.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/3142_1.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/801_4.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/4011_3.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/3705_3.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/3213_1.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/5597_3.txt')...]\n",
       "Path: /home/kienmn/.fastai/data/imdb\n",
       "Valid: TextList (10000 items)\n",
       "[Path('/home/kienmn/.fastai/data/imdb/train/neg/615_1.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/517_4.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/8027_1.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/8354_2.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/3355_4.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/11515_1.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/9151_1.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/9217_2.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/12065_3.txt'), Path('/home/kienmn/.fastai/data/imdb/train/neg/5490_1.txt')...]\n",
       "Path: /home/kienmn/.fastai/data/imdb"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))\n",
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "We need to tokenize the dataset first, which is splitting a sentence in individual tokens. Those tokens are the basic words or punctuation signs with a few tweaks: don't for instance is split between do and n't. We will use a processor for this, in conjunction with the [spacy library](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import spacy, html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before even tokenizeing, we will apply a bit of preprocessing on the texts to clean them up (we saw the one up there had some HTML code). These rules are applied before we split the sentences in tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# special tokens\n",
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "\n",
    "def sub_br(t):\n",
    "  \"Replaces the <br /> by \\n\"\n",
    "  re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "  return re_br.sub(\"\\n\", t)\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "  \"Add spaces around / and #\"\n",
    "  return re.sub(r'([/#])', r' \\1 ', t)\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "  \"Remove multiple spaces\"\n",
    "  return re.sub(' {2,}', ' ', t)\n",
    "\n",
    "def replace_rep(t):\n",
    "  \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n",
    "  def _replace_rep(m:Collection[str]) -> str:\n",
    "    c,cc = m.groups()\n",
    "    return f' {TK_REP} {len(cc)+1} {c} '\n",
    "  re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "  return re_rep.sub(_replace_rep, t)\n",
    "    \n",
    "def replace_wrep(t):\n",
    "  \"Replace word repetitions: word word word -> TK_WREP 3 word\"\n",
    "  def _replace_wrep(m:Collection[str]) -> str:\n",
    "    c,cc = m.groups()\n",
    "    return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
    "  re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "  return re_wrep.sub(_replace_wrep, t)\n",
    "\n",
    "def fixup_text(x):\n",
    "  \"Various messy things we've seen in documents\"\n",
    "  re1 = re.compile(r'  +')\n",
    "  x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "      'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "      '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "      ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "  return re1.sub(' ', html.unescape(x))\n",
    "    \n",
    "default_pre_rules = [fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br]\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xxrep 4 c '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_rep('cccc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xxwrep 5 word  '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_wrep('word word word word word ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rules are applies after the tokenization on the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_all_caps(x):\n",
    "  \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "  res = []\n",
    "  for t in x:\n",
    "    if t.isupper() and len(t) > 1:\n",
    "      res.append(TK_UP)\n",
    "      res.append(t.lower())\n",
    "    else:\n",
    "      res.append(t)\n",
    "  return res\n",
    "\n",
    "def deal_caps(x):\n",
    "  \"Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before.\"\n",
    "  res = []\n",
    "  for t in x:\n",
    "    if t == '':\n",
    "      continue\n",
    "    if t[0].isupper() and len(t) > 1 and t[1:].islower():\n",
    "      res.append(TK_MAJ)\n",
    "    res.append(t.lower())\n",
    "  return res\n",
    "\n",
    "def add_eos_bos(x):\n",
    "  return [BOS] + x + [EOS]\n",
    "\n",
    "default_post_rules = [deal_caps, replace_all_caps, add_eos_bos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'xxup', 'am', 'xxup', 'shouting']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_all_caps(['I', 'AM', 'SHOUTING'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxmaj', 'my', 'name', 'is', 'xxmaj', 'jeremy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deal_caps(['My', 'name', 'is', 'Jeremy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tokenizing and applying those rules takes a bit of time, we'll parallelize it using `ProcessPoolExecutor` to go faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from spacy.symbols import ORTH\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def parallel(func, arr, max_workers=4):\n",
    "  if max_workers < 2:\n",
    "    results = list(progress_bar(map(func, enumerate(arr)), total=len(arr)))\n",
    "  else:\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "      return list(progress_bar(ex.map(func, enumerate(arr)), total=len(arr)))\n",
    "  if any([o is not None for o in results]):\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenizeProcessor(Processor):\n",
    "  def __init__(self, lang='en', chunksize=2000, pre_rules=None, post_rules=None, max_workers=4):\n",
    "    self.chunksize = chunksize\n",
    "    self.max_workers = max_workers\n",
    "    self.tokenizer = spacy.blank(lang).tokenizer\n",
    "    for w in default_spec_tok:\n",
    "      self.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "    self.pre_rules = default_pre_rules if pre_rules is None else pre_rules\n",
    "    self.post_rules = default_post_rules if post_rules is None else post_rules\n",
    "    \n",
    "  def proc_chunk(self, args):\n",
    "    i, chunk = args\n",
    "    chunk = [compose(t, self.pre_rules) for t in chunk]\n",
    "    docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)]\n",
    "    docs = [compose(t, self.post_rules) for t in docs]\n",
    "    return docs\n",
    "  \n",
    "  def __call__(self, items):\n",
    "    toks = []\n",
    "    if isinstance(items[0], Path):\n",
    "      items = [read_file(i) for i in items]\n",
    "    chunks = [items[i: i + self.chunksize] for i in (range(0, len(items), self.chunksize))]\n",
    "    toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers)\n",
    "    return sum(toks, [])\n",
    "  \n",
    "  def proc1(self, item):\n",
    "    return self.proc_chunk([item])[0]\n",
    "  \n",
    "  def deprocess(self, toks):\n",
    "    return [self.deproc1(tok) for tok in toks]\n",
    "  \n",
    "  def deproc1(self, tok):\n",
    "    return \" \".join(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TokenizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've gotta say, I usually like horror movies that i've never seen... however, this one was just to pathetic for my gory taste. I'm used to the gory, gut wrenching types... but this particular movie was lame. The acting was horrible (yet the corny (no\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"xxbos • i • 've • got • ta • say • , • i • usually • like • horror • movies • that • i • 've • never • seen • ... • however • , • this • one • was • just • to • pathetic • for • my • gory • taste • . • i • 'm • used • to • the • gory • , • gut • wrenching • types • ... • but • this • particular • movie • was • lame • . • xxmaj • the • acting • was • horrible • ( • yet • the • corny • ( • no • pun \""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' • '.join(tp(il[:100])[0])[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalizing\n",
    "Once we have tokenized our texts, we replace each token by an individual number, this is called numericalizing. Again, we do this with a processor (not so different from the `CategoryProcessor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import collections\n",
    "\n",
    "class NumericalizeProcessor(Processor):\n",
    "  def __init__(self, vocab=None, max_vocab=60000, min_freq=2):\n",
    "    self.vocab = vocab\n",
    "    self.max_vocab = max_vocab\n",
    "    self.min_freq = min_freq\n",
    "    \n",
    "  def __call__(self, items):\n",
    "    # The vocab is defined on the first use.\n",
    "    if self.vocab is None:\n",
    "      freq = Counter(p for o in items for p in o)\n",
    "      self.vocab = [o for o, c in freq.most_common(self.max_vocab) if c >= self.min_freq]\n",
    "      for o in reversed(default_spec_tok):\n",
    "        if o in self.vocab:\n",
    "          self.vocab.remove(o)\n",
    "        self.vocab.insert(0, o)\n",
    "          \n",
    "    if getattr(self, 'otoi', None) is None:\n",
    "      self.otoi = collections.defaultdict(int, {v: k for k, v in enumerate(self.vocab)})\n",
    "    return [self.proc1(o) for o in items]\n",
    "  \n",
    "  def proc1(self, item):\n",
    "    return [self.otoi[o] for o in item]\n",
    "  \n",
    "  def deprocess(self, idxs):\n",
    "    assert self.vocab is not None\n",
    "    return [self.deproc1(idx) for idx in idxs]\n",
    "  \n",
    "  def deproc1(self, idx):\n",
    "    return [self.vocab[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do language modeling, we will infer the labels from the text during training, so there's no need to label. The training loop expects labels however, so we need to add dummy ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok = TokenizeProcessor(max_workers=8)\n",
    "proc_num = NumericalizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='45' class='' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [45/45 00:26<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:03<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 1.52 s, total: 12.8 s\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "%time ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok, proc_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the items have been processed they will become list of numbers, we can still access the underlying raw data in `x_obj` (or `y_obj` for the targets, but we don't have any here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj margret xxmaj laurence probably did n\\'t intend on having any of her novels adopted for film , let alone the xxmaj stone xxmaj angel . xxmaj hagar , as a character , was one who constantly challenged the social norm ( xxmaj xxunk who dare , anyone ? ) , and ended up nearly sacrificing her humanity in the process . xxmaj the symbols in the book ( the xxmaj stone xxmaj angel , xxmaj silver xxmaj thread , etc , etc . ) are constant reminders of this struggle of the old and new , and the carnage ( so to speak ) along the way . \\n\\n xxmaj while the film is reasonably faithful to the plot of the book ( but it is n\\'t really a plot kind - of storytelling , is it ? ) , i think it missed the point on capturing the spirit of the film . xxmaj hagar \\'s defiance ( for the sake of defiance ) was not there . xxmaj bram could have been a lot more crude than portrayed , and xxmaj hagar \\'s father could have been played more \" traditionally \" , so to speak . xxmaj if the filmmaker would insisted on stronger portrayals , the film would drive the point straight to home . \\n\\n xxmaj along the same vein , why should we see cell phones , organic produce , and other modernizations ? xxmaj are we trying make some points for the sake of making some points ( e.g. , the xxmaj muslim girlfriend and the xxmaj native people ) . xxmaj hagar and co. are everything but politically correct in the book , so why should we see that in the film version . xxmaj modernization may be an excuse for a low - budget operation , but using that as an excuse to send subliminal politically - correct messages that are totally irrelevant to the novel ( and the film ) seems like throwing punches below the intellect . \\n\\n xxmaj there is also the audience . xxmaj it seems that we have been conditioned to see bitter old people as cute and lovable . xxmaj why should be laugh every time xxmaj hagar is at her tantrums ? i doubt xxmaj xxunk xxmaj laurence wanted her readers to laugh at , or with , xxmaj hagar . xxmaj these people are frustrated and are full of angst , and all we do is to laugh at them . i do n\\'t think it did xxmaj hagar and other folks in her situation any justice . xxeos'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.train.x_obj(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the preprocessing takes time, we save the intermediate result using pickle. Don't use any lambda functions in your processors or they won't be able to pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ld.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ld.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "We have a bit of work to convert our `LabelList` in a `DataBunch` as we don't just want batches of IMDB reviews. We want to stream through all the texts concatenated. We also have to prepare the targets that are the next words in the text. All of this is done with the next object called `LM_PreLoader`. At the beginning of each epoch, it'll shuffle the articles (if `shuffle=True`) and create a big stream by concatenating all of them. We divide this big stream in `bs` smaller streams. That we will read in chunks of bptt length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just using those for illustration purposes, they're not used otherwise.\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say our stream is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = \"\"\"\n",
    "In this notebook, we will go back over the example of classifying movie reviews we studied in part 1 and dig deeper under the surface. \n",
    "First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the Processor used in the data block API.\n",
    "Then we will study how we build a language model and train it.\\n\n",
    "\"\"\"\n",
    "tokens = np.array(tp([stream])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['xxbos', '\\n', 'xxmaj', 'in', 'this', 'notebook', ',', 'we',\n",
       "       'will', 'go', 'back', 'over', 'the', 'example', 'of',\n",
       "       'classifying', 'movie', 'reviews', 'we', 'studied', 'in', 'part',\n",
       "       '1', 'and', 'dig', 'deeper', 'under', 'the', 'surface', '.', '\\n',\n",
       "       'xxmaj', 'first', 'we', 'will', 'look', 'at', 'the', 'processing',\n",
       "       'steps', 'necessary', 'to', 'convert', 'text', 'into', 'numbers',\n",
       "       'and', 'how', 'to', 'customize', 'it', '.', 'xxmaj', 'by', 'doing',\n",
       "       'this', ',', 'we', \"'ll\", 'have', 'another', 'example', 'of',\n",
       "       'the', 'xxmaj', 'processor', 'used', 'in', 'the', 'data', 'block',\n",
       "       'api', '.', '\\n', 'xxmaj', 'then', 'we', 'will', 'study', 'how',\n",
       "       'we', 'build', 'a', 'language', 'model', 'and', 'train', 'it', '.',\n",
       "       '\\n\\n', 'xxeos'], dtype='<U11')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we split it in 6 batches it would give something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "      <td>notebook</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "      <td>back</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classifying</td>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "      <td>in</td>\n",
       "      <td>part</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "      <td>deeper</td>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>processor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "      <td>block</td>\n",
       "      <td>api</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1        2      3          4          5      6      7  \\\n",
       "0        xxbos       \\n    xxmaj     in       this   notebook      ,     we   \n",
       "1  classifying    movie  reviews     we    studied         in   part      1   \n",
       "2           \\n    xxmaj    first     we       will       look     at    the   \n",
       "3      numbers      and      how     to  customize         it      .  xxmaj   \n",
       "4      another  example       of    the      xxmaj  processor   used     in   \n",
       "5         then       we     will  study        how         we  build      a   \n",
       "\n",
       "            8      9         10     11       12       13     14  \n",
       "0        will     go       back   over      the  example     of  \n",
       "1         and    dig     deeper  under      the  surface      .  \n",
       "2  processing  steps  necessary     to  convert     text   into  \n",
       "3          by  doing       this      ,       we      'll   have  \n",
       "4         the   data      block    api        .       \\n  xxmaj  \n",
       "5    language  model        and  train       it        .   \\n\\n  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 6\n",
    "seq_len = 15\n",
    "\n",
    "d_tokens = np.array([tokens[i * seq_len: (i + 1) * seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "# display(HTML(df.to_html(index=False, header=None)))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we have a `bptt` of 5, we would go over those three batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>classifying</td>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>notebook</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>part</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>processor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>back</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>deeper</td>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>block</td>\n",
       "      <td>api</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs = 6\n",
    "bptt = 5\n",
    "for k in range(3):\n",
    "  d_tokens = np.array([tokens[i * seq_len + k * bptt: i * seq_len + (k + 1) * bptt] for i in range(bs)])\n",
    "  df = pd.DataFrame(d_tokens)\n",
    "  display(HTML(df.to_html(index=False, header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LM_PreLoader():\n",
    "  def __init__(self, data, bs=64, bptt=70, shuffle=False):\n",
    "    self.data = data\n",
    "    self.bs = bs\n",
    "    self.bptt = bptt\n",
    "    self.shuffle = shuffle\n",
    "    total_len = sum([len(t) for t in data.x])\n",
    "    self.n_batch = total_len // bs\n",
    "    self.batchify()\n",
    "    \n",
    "  def __len__(self):\n",
    "    return ((self.n_batch - 1) // self.bptt) * self.bs\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    source = self.batched_data[idx % self.bs]\n",
    "    seq_idx = (idx // self.bs) * self.bptt\n",
    "    return source[seq_idx: seq_idx + self.bptt], source[seq_idx + 1: seq_idx + self.bptt + 1]\n",
    "  \n",
    "  def batchify(self):\n",
    "    texts = self.data.x\n",
    "    if self.shuffle:\n",
    "      texts = texts[torch.randperm(len(texts))]\n",
    "    stream = torch.cat([tensor(t) for t in texts])\n",
    "    self.batched_data = stream[: self.n_batch * self.bs].view(self.bs, self.n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(LM_PreLoader(ll.valid, shuffle=True), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it all works ok: `x1`, `y1`, `x2` and `y2` should all be of size `bs`  by `bptt`. The texts in each row of `x1` should continue in `x2`. `y1` and `y2` should have the same texts as their `x` counterpart, shifted of one position to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(dl)\n",
    "x1, y1 = next(iter_dl)\n",
    "x2, y2 = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70]), torch.Size([64, 70]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.size(), y1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = proc_num.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj explorers is yet another gem from the criminally underrated xxmaj joe xxmaj dante . xxmaj after xxmaj dante struck box office gold with xxmaj gremlins , nothing else he has done since has achieved the same level of success . xxmaj explorers was the first film he made after xxmaj gremlins , but it failed to strike a chord with audiences . a shame really because i think'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in x1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxmaj explorers is yet another gem from the criminally underrated xxmaj joe xxmaj dante . xxmaj after xxmaj dante struck box office gold with xxmaj gremlins , nothing else he has done since has achieved the same level of success . xxmaj explorers was the first film he made after xxmaj gremlins , but it failed to strike a chord with audiences . a shame really because i think it'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in y1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it 's one of xxmaj dante 's best films . xxmaj slightly flawed perhaps , but it 's a fairly intelligent kids film , not to mention quite charming . \\n\\n xxmaj explorers was one of a series of films that followed in the wake of et . xxmaj but i 've never really been much of a fan of et . xxmaj it seemed a little too calculated to\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in x2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's prepare some convenience function to do this quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):\n",
    "  return (DataLoader(LM_PreLoader(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs),\n",
    "          DataLoader(LM_PreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2 * bs, **kwargs))\n",
    "\n",
    "def lm_databunchify(sd, bs, bptt, **kwargs):\n",
    "  return DataBunch(*get_lm_dls(sd.train, sd.valid, bs, bptt, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "bptt = 70\n",
    "data = lm_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching for classification\n",
    "When we will want to tackle classification, gathering the data will be a bit different: first we will label our texts with the folder they come from, and then we will need to apply padding to batch them together. To avoid mixing very long texts with very short ones, we will also use `Sampler` to sort (with a bit of randomness for the training set) our samples by length.\n",
    "\n",
    "First the data block API calls shold look familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_cat = CategoryProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:09<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:08<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test'])\n",
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))\n",
    "ll = label_by_func(sd, parent_labeler, proc_x=[proc_tok, proc_num], proc_y=proc_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ll_clas.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the labels seem consitent with the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxbos xxmaj margret xxmaj laurence probably did n\\'t intend on having any of her novels adopted for film , let alone the xxmaj stone xxmaj angel . xxmaj hagar , as a character , was one who constantly challenged the social norm ( xxmaj xxunk who dare , anyone ? ) , and ended up nearly sacrificing her humanity in the process . xxmaj the symbols in the book ( the xxmaj stone xxmaj angel , xxmaj silver xxmaj thread , etc , etc . ) are constant reminders of this struggle of the old and new , and the carnage ( so to speak ) along the way . \\n\\n xxmaj while the film is reasonably faithful to the plot of the book ( but it is n\\'t really a plot kind - of storytelling , is it ? ) , i think it missed the point on capturing the spirit of the film . xxmaj hagar \\'s defiance ( for the sake of defiance ) was not there . xxmaj bram could have been a lot more crude than portrayed , and xxmaj hagar \\'s father could have been played more \" traditionally \" , so to speak . xxmaj if the filmmaker would insisted on stronger portrayals , the film would drive the point straight to home . \\n\\n xxmaj along the same vein , why should we see cell phones , organic produce , and other modernizations ? xxmaj are we trying make some points for the sake of making some points ( e.g. , the xxmaj muslim girlfriend and the xxmaj native people ) . xxmaj hagar and co. are everything but politically correct in the book , so why should we see that in the film version . xxmaj modernization may be an excuse for a low - budget operation , but using that as an excuse to send subliminal politically - correct messages that are totally irrelevant to the novel ( and the film ) seems like throwing punches below the intellect . \\n\\n xxmaj there is also the audience . xxmaj it seems that we have been conditioned to see bitter old people as cute and lovable . xxmaj why should be laugh every time xxmaj hagar is at her tantrums ? i doubt xxmaj xxunk xxmaj laurence wanted her readers to laugh at , or with , xxmaj hagar . xxmaj these people are frustrated and are full of angst , and all we do is to laugh at them . i do n\\'t think it did xxmaj hagar and other folks in her situation any justice . xxeos',\n",
       "  'neg'),\n",
       " (\"xxbos a fantastic xxmaj arabian adventure . a former king , xxmaj ahmad , and his best friend , the thief xxmaj abu ( played by xxmaj sabu of xxmaj black xxmaj narcissus ) search for xxmaj ahmad 's love interest , who has been stolen by the new king , xxmaj jaffar ( xxmaj conrad xxmaj veidt ) . xxmaj there 's hardly a down moment here . xxmaj it 's always inventing new adventures for the heroes . xxmaj personally , i found xxmaj ahmad and his princess a little boring ( there 's no need to ask why xxmaj john xxmaj justin , who plays xxmaj ahmad , is listed fourth in the credits ) . xxmaj conrad xxmaj veidt , always a fun actor , makes a great villain , and xxmaj sabu is a lot of fun as the prince of thieves , who at one point finds a genie in a bottle . i also really loved xxmaj miles xxmaj malleson as the xxmaj sultan of xxmaj basra , the father of the princess . xxmaj he collects amazing toys from around the world . xxmaj jaffar bribes him for his daughter 's hand with a mechanical flying horse . xxmaj this probably would count as one of the great children 's films of all time , but the special effects are horribly dated nowadays . xxmaj kids will certainly deride the superimposed images when xxmaj abu and the genie are on screen together . xxmaj and the scene with the giant spider looks especially awful . xxmaj although most of the younger generation probably thinks that xxmaj king xxmaj kong looks bad at this point in time , xxmaj willis o'brien 's stop - motion animation is a thousand times better than a puppet on a string that does n't even look remotely like a spider . 8 / 10 . xxeos\",\n",
       "  'pos')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ll.train.x_obj(i), ll.train.y_obj(i)) for i in [1, 12552]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw samplers in notebook 03. For the validation set, we will simply sort the samples by length, and we begin with the longest ones for memory reasons (it's better to always have the biggest tensors first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class SortSampler(Sampler):\n",
    "  def __init__(self, data_source, key):\n",
    "    self.data_source = data_source\n",
    "    self.key = key\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data_source)\n",
    "  \n",
    "  def __iter__(self):\n",
    "    return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training set, we want some kind of randomness on top of this. So first, we shuffle the texts and build megabatches of size `50 * bs`. We sort those megabatches by length before splitting them in 50 minibatches. That way we will have randomized batches of roughly the same length.\n",
    "\n",
    "Then we make sure to have the biggest batch first and shuffle the order of the other batches. We also make sure the last batch stays at the end because its size is probably lower than batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SortishSampler(Sampler):\n",
    "  def __init__(self, data_source, key, bs):\n",
    "    self.data_source = data_source\n",
    "    self.key = key\n",
    "    self.bs = bs\n",
    "    \n",
    "  def __len__(self) -> int:\n",
    "    return len(self.data_source)\n",
    "  \n",
    "  def __iter__(self):\n",
    "    idxs = torch.randperm(len(self.data_source))\n",
    "    megabatches = [idxs[i: i + self.bs * 50] for i in range(0, len(idxs), self.bs * 50)]\n",
    "    sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches])\n",
    "    batches = [sorted_idx[i: i + self.bs] for i in range(0, len(sorted_idx), self.bs)]\n",
    "    max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))    # Find the chunk with the largest key,\n",
    "    batches[0], batches[max_idx] = batches[max_idx], batches[0]            # Then make sure it goes firts.\n",
    "    batch_idxs = torch.randperm(len(batches) - 2)\n",
    "    sorted_idx = torch.cat([batches[i + 1] for i in batch_idxs]) if len(batches) > 1 else LongTensor([])\n",
    "    sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n",
    "    return iter(sorted_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding: we had the padding token (that as an id of 1) at the end of each sequence to make them all the same size when batching them. Note that we need padding at the end to be able to use `PyTorch` convenience functions that will let us ignore that padding (see 12c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_collate(samples, pad_idx=1, pad_first=False):\n",
    "  max_len = max([len(s[0]) for s in samples])\n",
    "  res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "  for i, s in enumerate(samples):\n",
    "    if pad_first:\n",
    "      res[i, -len(s[0]):] = LongTensor(s[0])\n",
    "    else:\n",
    "      res[i, : len(s[0])] = LongTensor(s[0])\n",
    "  return res, tensor([s[1] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "train_sampler = SortishSampler(ll.train.x, key=lambda t: len(ll.train[int(t)][0]), bs=bs)\n",
    "train_dl = DataLoader(ll.train, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3311, 1702, 1521, 1443, 1333], 1024)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "for i in range(x.size(0)):\n",
    "  lengths.append(x.size(1) - (x[i] == 1).sum().item())\n",
    "lengths[:5], lengths[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last one is the minimal length. This is the first batch so it has the longest sequence, but if look at the next one that is more random, we see lengths are roughly the sames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([153, 153, 153, 153, 153], 149)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y =  next(iter_dl)\n",
    "lengths = []\n",
    "for i in range(x.size(0)):\n",
    "  lengths.append(x.size(1) - (x[i] == 1).sum().item())\n",
    "lengths[:5], lengths[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the padding at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,   18, 1055,  ...,   16,    9,    3],\n",
       "        [   2,    7,  190,  ...,   21,    9,    3],\n",
       "        [   2,    7,   25,  ..., 6556,    9,    3],\n",
       "        ...,\n",
       "        [   2,    7,   19,  ...,    1,    1,    1],\n",
       "        [   2,    7, 5518,  ...,    1,    1,    1],\n",
       "        [   2,   12,   84,  ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can add a convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_clas_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "  train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs)\n",
    "  valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))\n",
    "  return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs),\n",
    "          DataLoader(valid_ds, batch_size=bs * 2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs))\n",
    "\n",
    "def clas_databunchify(sd, bs, **kwargs):\n",
    "  return DataBunch(*get_clas_dls(sd.train, sd.valid, bs, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt = 64, 70\n",
    "data = clas_databunchify(ll, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12_text.ipynb to nb_12.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 12_text.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch Env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
